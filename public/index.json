[{"authors":["admin"],"categories":null,"content":"Jennifer is a PhD candidate in the Neuroscience Graduate Program at Oregon Health \u0026amp; Science University. Under the mentorship of Dr. Kevin Wright she studies the role that the scaffolding protein dystroglycan plays in the structure and function of inhibitory synapses in the brain. Her goal is to further our understanding of the cognitive symptoms that are experienced by dystroglycanopathy patients. Jennifer is also interested in data science and science communication. In her free time Jennifer can be found hiking, crocheting, making art, and petting dogs.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/jennifer-jahncke/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jennifer-jahncke/","section":"authors","summary":"Jennifer is a PhD candidate in the Neuroscience Graduate Program at Oregon Health \u0026amp; Science University. Under the mentorship of Dr. Kevin Wright she studies the role that the scaffolding protein dystroglycan plays in the structure and function of inhibitory synapses in the brain.","tags":null,"title":"Jennifer Jahncke","type":"authors"},{"authors":[],"categories":[],"content":"\r\rPresentation:\r\rHear me talk through the making of this visualization. All code featured in the presentation (and more!) can be found in the sections below.\r\r\rThe Data:\rDespite the frequency with which incidents of police brutality occurs in the US, an official centralized record of police violence does not exist. In 2015 The Washington Post took matters into their own hands and created a database of every fatal shooting in the US by a police officer. The record is regularly updated (right now it is current as of June 8, 2020). Currently there are 5401 fatalities represented in the dataset. Here is just a glimpse at the data:\n## # A tibble: 5,401 x 12\r## name date armed age gender race city state threat_level flee ## \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt;\r## 1 Tim ~ 2015-01-02 gun 53 M A Shel~ WA attack Not ~\r## 2 Lewi~ 2015-01-02 gun 47 M W Aloha OR attack Not ~\r## 3 John~ 2015-01-03 unar~ 23 M H Wich~ KS other Not ~\r## 4 Matt~ 2015-01-04 toy ~ 32 M W San ~ CA attack Not ~\r## 5 Mich~ 2015-01-04 nail~ 39 M H Evans CO attack Not ~\r## 6 Kenn~ 2015-01-04 gun 18 M W Guth~ OK attack Not ~\r## 7 Kenn~ 2015-01-05 gun 22 M H Chan~ AZ attack Car ## 8 Broc~ 2015-01-06 gun 35 M W Assa~ KS attack Not ~\r## 9 Autu~ 2015-01-06 unar~ 34 F W Burl~ IA other Not ~\r## 10 Lesl~ 2015-01-06 toy ~ 47 M B Knox~ PA attack Not ~\r## # ... with 5,391 more rows, and 2 more variables: body_camera \u0026lt;lgl\u0026gt;,\r## # signs_of_mental_illness \u0026lt;lgl\u0026gt;\rWhile the Washington Post data allows us to see raw number of shootings that occur, if we want to see how Black and White people are differentially targeted by police we are going to need to calculate the proportion of Black/White people that are shot. To do this I need data on the population size of Black and White people living in the US. I got this data from the US Census Bureau.\nExplanation of Variables in Washington Post dataset\rQualitative variables: name, armed, gender, race, city, state, signs_of_mental_illness, threat_level, flee, body_camera\nQuantitative variables: age\narmed: Did the victim have a weapon? If yes, what kind? There are 89 weapons represented in the dataset, ranging from a gun to a chair.\nrace:\n\rW = White, non-Hispanic (2468)\n\rB = Black, non-Hispanic (1291)\n\rA = Asian (93)\n\rN = Native American (78)\n\rH = Hispanic (900)\n\rO = Other (48)\n\rNA = Unknown (523)\r\rsigns_of_mental_illness: Did the victim exhibit signs of mental illness?\nTRUE = 1216\nFALSE = 4185\nthreat_level: Was there a direct and immediate threat to the life of the police officer? This includes incidents where officers or others were shot at, threatened with a gun, attacked with other weapons or physical force, etc.\nattack = 3487\nother = 1914\nflee: Was the victim moving away from the officers?\nNot fleeing = 3406\nCar = 898\nFoot = 689\nOther = 162\nNA = 246\nbody_camera: Reported as TRUE if news reports indicated an officer was wearing a body camera and it may have recorded at least a portion of the incident.\nTRUE = 615\nFALSE = 4796\n\rLinks to datasets\r\rShooting fatality data c/o Washington Post\rPopulation data c/o US Census Bureau\r\r\r\rThe Visualization:\rIntended Audience\rIt’s never fun to talk about violence, especially violence present in a system that is supposed to hold honor, however I think that it is important that everyone be made aware of the racial bias that is reflected in statistics describing police brutality. This audience for this visualization is therefore broad: people of all ages, genders, education levels, socioeconomic status, etc. This plot is easily understood even without previous experience with dumbbell plots.\n\rAbout Dumbbell Plots\rAlso called connected dot plots or dumbbell dot plots, dumbbell plots are a version of lollipop charts that features comparison between 2 (or 3) groups. Lollipop charts are closely related to bar charts but are only effective in conveying information about a single group. By using the dumbbell layout you can increase the depth of information conveyed. While a grouped bar chart would also convey information about the two groups, dumbbell plots take advantage of the Gestalt principle of continuity to aid the eye in following the directionality of the relationship.\n\rHow to Read it and What to Look For\rDumbbell plots consist of 2 (or 3) points connected by a line. Often there are multiple “dumbbells” to represent different groups, timepoints, etc. The points indicate the numerical (or categorical) value for a group. The line connecting two points exists to indicate the relationship between the two points, both in directionality and magnitude. It also functions to guide the eye in appropriate grouping of points. In my visualization I am trying to convey the relationship between Black and White victims of fatal shootings. The x-axis carries information about the number of victims. The y-axis represents time, in years. We can then follow the incidence of shootings across time for both races.\n\rRepresentation Description/Intended Message\rMy goal for this visualization was to illustrate how Black and White populations are targetd differently by gun violence, specifically gun violence in the contect of fatal line-of-duty police shootings. To do this, I cannot present the raw number of shootings in each population becuase the population sizes are vastly different (Black people are a minority, after all). Instead, I’m showing the number of fatal shootings per 1 million people of a given race (ie. the proportion of each population that dies due to gun violence). Below is a table of those numbers:\n\r\r\rTotal Number of\nFatal Shootings\r\r\rTotal Population\n(in Millions)\r\r\rDeaths per 1 Million People of Indicated Race\r\r\r\r\rYear\r\rWhite\r\rBlack\r\rWhite\r\rBlack\r\rWhite\r\rBlack\r\r\r\r\r\r2015\r\r497.00\r\r258.00\r\r247.78\r\r42.63\r\r2.01\r\r6.05\r\r\r\r2016\r\r468.00\r\r234.00\r\r248.50\r\r43.00\r\r1.88\r\r5.44\r\r\r\r2017\r\r459.00\r\r224.00\r\r249.62\r\r43.50\r\r1.84\r\r5.15\r\r\r\r2018\r\r454.00\r\r229.00\r\r250.14\r\r43.80\r\r1.81\r\r5.23\r\r\r\r2019\r\r405.00\r\r249.00\r\r249.42\r\r43.43\r\r1.62\r\r5.73\r\r\r\r2020\r\r424.69\r\r222.67\r\r249.73\r\r43.58\r\r1.70\r\r5.11\r\r\r\r\r\r* 2020 data is projected based on current data.\r\r\r\r† Data is current as of June 8, 2020.\r\r\r\r\rFrom both the table and the visualization it is clear that Black people are consistently disproportionately targeted by gun violence in this context. This holds up over all years for which there is data available.\n\rPresentation\rI chose to make this plot in portrait orientation (vs landscape) because I thought it made the relationships more readily interpretable. I removed major and minor grids on the y-axis since I found them distracting and the segment connecting the two points already provides the same information. While years are technically quantitative, the way I’m using them makes them almost categorical and thus it is less important to have grid lines. I did keep grid lines for the x-axis to help interpretation of the value of the points. The color choices were deliberate and were chosen to represent skin color. A text and arrow annotation was used to indicate that the 2020 data is projected based on current data. A footnote was used to indicate how current the data is. I positioned the legend under the title such that it is readily available but doesn’t occupy too much space. For the title I used a bold-face font for the title and regular font for the subtitle. I found that by using that bold font I was able to better visually separate the title and subtitle.\n\rHow I created it\r1. Import and wrangle the shooting data. \nshootings_raw \u0026lt;- read_csv(\u0026quot;fatal-police-shootings-data.csv\u0026quot;)\rglimpse(shootings_raw)\r## Rows: 5,401\r## Columns: 14\r## $ id \u0026lt;dbl\u0026gt; 3, 4, 5, 8, 9, 11, 13, 15, 16, 17, 19, 21, ...\r## $ name \u0026lt;chr\u0026gt; \u0026quot;Tim Elliot\u0026quot;, \u0026quot;Lewis Lee Lembke\u0026quot;, \u0026quot;John Pau...\r## $ date \u0026lt;date\u0026gt; 2015-01-02, 2015-01-02, 2015-01-03, 2015-0...\r## $ manner_of_death \u0026lt;chr\u0026gt; \u0026quot;shot\u0026quot;, \u0026quot;shot\u0026quot;, \u0026quot;shot and Tasered\u0026quot;, \u0026quot;shot\u0026quot;,...\r## $ armed \u0026lt;chr\u0026gt; \u0026quot;gun\u0026quot;, \u0026quot;gun\u0026quot;, \u0026quot;unarmed\u0026quot;, \u0026quot;toy weapon\u0026quot;, \u0026quot;nai...\r## $ age \u0026lt;dbl\u0026gt; 53, 47, 23, 32, 39, 18, 22, 35, 34, 47, 25,...\r## $ gender \u0026lt;chr\u0026gt; \u0026quot;M\u0026quot;, \u0026quot;M\u0026quot;, \u0026quot;M\u0026quot;, \u0026quot;M\u0026quot;, \u0026quot;M\u0026quot;, \u0026quot;M\u0026quot;, \u0026quot;M\u0026quot;, \u0026quot;M\u0026quot;, \u0026quot;F\u0026quot;...\r## $ race \u0026lt;chr\u0026gt; \u0026quot;A\u0026quot;, \u0026quot;W\u0026quot;, \u0026quot;H\u0026quot;, \u0026quot;W\u0026quot;, \u0026quot;H\u0026quot;, \u0026quot;W\u0026quot;, \u0026quot;H\u0026quot;, \u0026quot;W\u0026quot;, \u0026quot;W\u0026quot;...\r## $ city \u0026lt;chr\u0026gt; \u0026quot;Shelton\u0026quot;, \u0026quot;Aloha\u0026quot;, \u0026quot;Wichita\u0026quot;, \u0026quot;San Francis...\r## $ state \u0026lt;chr\u0026gt; \u0026quot;WA\u0026quot;, \u0026quot;OR\u0026quot;, \u0026quot;KS\u0026quot;, \u0026quot;CA\u0026quot;, \u0026quot;CO\u0026quot;, \u0026quot;OK\u0026quot;, \u0026quot;AZ\u0026quot;, \u0026quot;...\r## $ signs_of_mental_illness \u0026lt;lgl\u0026gt; TRUE, FALSE, FALSE, TRUE, FALSE, FALSE, FAL...\r## $ threat_level \u0026lt;chr\u0026gt; \u0026quot;attack\u0026quot;, \u0026quot;attack\u0026quot;, \u0026quot;other\u0026quot;, \u0026quot;attack\u0026quot;, \u0026quot;att...\r## $ flee \u0026lt;chr\u0026gt; \u0026quot;Not fleeing\u0026quot;, \u0026quot;Not fleeing\u0026quot;, \u0026quot;Not fleeing\u0026quot;...\r## $ body_camera \u0026lt;lgl\u0026gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, F...\rIf we take a quick look at how it was imported, you can see that a lot of the variables were imported as character class when they should operate as factor class. I’ll use forcats::as_factor() to turn those variables to factor class. I also need to create a new “Year” column since I am interested in the number of shootings per year.\nshootings_raw \u0026lt;- shootings_raw %\u0026gt;% mutate(manner_of_death = as_factor(manner_of_death),\rarmed = as_factor(armed),\rgender = as_factor(gender),\rrace = as_factor(race),\rstate = as_factor(state),\rthreat_level = as_factor(threat_level),\rflee = as_factor(flee))\r# Extract \u0026quot;Year\u0026quot; from \u0026quot;date\u0026quot;\rshootings_raw$Year \u0026lt;- as.numeric(format(shootings_raw$date, \u0026#39;%Y\u0026#39;))\rNow I’m going to use summarise() to calculate the number of shootings per year for each race. Then I’m going to filter the data to only look at data for Black and White victims.\n# Count W/B shootings per year\rshootings \u0026lt;- shootings_raw %\u0026gt;% group_by(Year, race) %\u0026gt;% summarise(fatalshootings = n()) %\u0026gt;% filter(race == \u0026quot;W\u0026quot; | race == \u0026quot;B\u0026quot;)\r## `summarise()` regrouping output by \u0026#39;Year\u0026#39; (override with `.groups` argument)\rshootings %\u0026gt;% head(12)\r## # A tibble: 12 x 3\r## # Groups: Year [6]\r## Year race fatalshootings\r## \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt;\r## 1 2015 W 497\r## 2 2015 B 258\r## 3 2016 W 468\r## 4 2016 B 234\r## 5 2017 W 459\r## 6 2017 B 224\r## 7 2018 W 454\r## 8 2018 B 229\r## 9 2019 W 405\r## 10 2019 B 249\r## 11 2020 W 185\r## 12 2020 B 97\rIn order to combine this with my census data, I’m going to need to pivot the data wider such that each row represents a single year.\nshootings \u0026lt;- shootings %\u0026gt;% pivot_wider(names_from = race, values_from = fatalshootings)\rshootings\r## # A tibble: 6 x 3\r## # Groups: Year [6]\r## Year W B\r## \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt;\r## 1 2015 497 258\r## 2 2016 468 234\r## 3 2017 459 224\r## 4 2018 454 229\r## 5 2019 405 249\r## 6 2020 185 97\r2. Import and wrangle the census data.\n# How many B/W people are in America from 2015-2020?\rcensus_raw \u0026lt;- read_xlsx(\u0026quot;census-data.xlsx\u0026quot;)\r# Convert units from millions\rcensus \u0026lt;- census_raw %\u0026gt;% mutate(W_pop = White * 1000000,\rB_pop = Black * 1000000)\rcensus\r## # A tibble: 21 x 5\r## Year White Black W_pop B_pop\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2020 250. 43.6 249726667. 43577778.\r## 2 2019 249. 43.4 249420000 43433333.\r## 3 2018 250. 43.8 250140000 43800000 ## 4 2017 250. 43.5 249620000 43500000 ## 5 2016 248. 43 248500000 43000000 ## 6 2015 248. 42.6 247780000 42630000 ## 7 2014 247. 42.2 246660000 42160000 ## 8 2013 246. 41.7 245590000 41710000 ## 9 2012 245. 41.3 244510000 41260000 ## 10 2011 243. 40.8 243380000 40810000 ## # ... with 11 more rows\r3. Combine the census data with the shooting data.\n# Combine shooting \u0026amp; census data\rshootings \u0026lt;- inner_join(shootings, census, by=\u0026quot;Year\u0026quot;) %\u0026gt;% rename(W_mil = White,\rB_mil = Black) %\u0026gt;% # Calculate the number of shootings per 1 million people\rmutate(W_per = W*1000000/W_pop,\rB_per = B*1000000/B_pop)\r4. For 2020: calculate the projected numbers. I’ll do this by (1) calculating the number of days represented in the data and (2) dividing my values by the number of days and multiply by 365.\n# Calculate projected data for 2020\r# How many days of data is represented for 2020 data? Latest date is 06/08/2020\rdays \u0026lt;- as.numeric(as.Date(as.character(\u0026quot;2020/06/08\u0026quot;), format=\u0026quot;%Y/%m/%d\u0026quot;)-as.Date(as.character(\u0026quot;2020/01/01\u0026quot;), format=\u0026quot;%Y/%m/%d\u0026quot;))\r# Calculate projected shootings for 2020\rproj2020 \u0026lt;- shootings[6,]\rproj2020 \u0026lt;- tibble(Year = \u0026quot;2020 proj\u0026quot;,\rW = pull(proj2020[1,2])*365/days,\rB = pull(proj2020[1,3])*365/days,\rW_mil = pull(proj2020[1,4]),\rB_mil = pull(proj2020[1,5]), W_pop = pull(proj2020[1,6]),\rB_pop = pull(proj2020[1,7]),\rW_per = pull(proj2020[1,8])*365/days,\rB_per = pull(proj2020[1,9])*365/days) proj2020\r## # A tibble: 1 x 9\r## Year W B W_mil B_mil W_pop B_pop W_per B_per\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2020 proj 425. 223. 250. 43.6 249726667. 43577778. 1.70 5.11\r# Add projected values to shootings data frame\rshootings \u0026lt;- shootings %\u0026gt;% ungroup() %\u0026gt;% mutate(Year = as.character(Year))\rshootings \u0026lt;- bind_rows(shootings, proj2020)\rshootings\r## # A tibble: 7 x 9\r## Year W B W_mil B_mil W_pop B_pop W_per B_per\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2015 497 258 248. 42.6 247780000 42630000 2.01 6.05\r## 2 2016 468 234 248. 43 248500000 43000000 1.88 5.44\r## 3 2017 459 224 250. 43.5 249620000 43500000 1.84 5.15\r## 4 2018 454 229 250. 43.8 250140000 43800000 1.81 5.23\r## 5 2019 405 249 249. 43.4 249420000 43433333. 1.62 5.73\r## 6 2020 185 97 250. 43.6 249726667. 43577778. 0.741 2.23\r## 7 2020 proj 425. 223. 250. 43.6 249726667. 43577778. 1.70 5.11\r5. Time to get the data into the format we need for ggplot. First, I need to pivot the data longer. Then I can get rid of the 2020 data and replace it with the projected data.\n# Pivot long\rshootings_long \u0026lt;- shootings %\u0026gt;% pivot_longer(W_per:B_per, names_to = \u0026quot;race\u0026quot;, values_to = \u0026quot;fatal_per\u0026quot;)\r# Get rid of 2020, replace it with the projected numbers\rshootings \u0026lt;- shootings %\u0026gt;% filter(Year != \u0026quot;2020\u0026quot;)\rshootings[6,1] = \u0026quot;2020\u0026quot;\rshootings_long \u0026lt;- shootings_long %\u0026gt;% filter(Year != \u0026quot;2020\u0026quot;)\rshootings_long[11,1] = \u0026quot;2020\u0026quot;; shootings_long[12,1] = \u0026quot;2020\u0026quot;\rshootings_long\r## # A tibble: 12 x 9\r## Year W B W_mil B_mil W_pop B_pop race fatal_per\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2015 497 258 248. 42.6 247780000 42630000 W_per 2.01\r## 2 2015 497 258 248. 42.6 247780000 42630000 B_per 6.05\r## 3 2016 468 234 248. 43 248500000 43000000 W_per 1.88\r## 4 2016 468 234 248. 43 248500000 43000000 B_per 5.44\r## 5 2017 459 224 250. 43.5 249620000 43500000 W_per 1.84\r## 6 2017 459 224 250. 43.5 249620000 43500000 B_per 5.15\r## 7 2018 454 229 250. 43.8 250140000 43800000 W_per 1.81\r## 8 2018 454 229 250. 43.8 250140000 43800000 B_per 5.23\r## 9 2019 405 249 249. 43.4 249420000 43433333. W_per 1.62\r## 10 2019 405 249 249. 43.4 249420000 43433333. B_per 5.73\r## 11 2020 425. 223. 250. 43.6 249726667. 43577778. W_per 1.70\r## 12 2020 425. 223. 250. 43.6 249726667. 43577778. B_per 5.11\r6. Build the plot in ggplot. A dumbbell plot can be made in ggplot using a combination of geom_segment() and geom_point(). Let’s look at the most basic version. Note that I’m using data from shootings for geom_segment() and data from shootings_long for geom_point(). Note that I’m already implementing some customizations. I’ve specified the size of line to use for the segment. I’ve also specified the size, shape, and outline color for the points.\nggplot() +\rgeom_segment(data = shootings, mapping = aes(x=W_per, xend=B_per, y=Year, yend=Year), size = 1) +\rgeom_point(data = shootings_long, mapping = aes(x = fatal_per, y = Year, fill = race), size=5, shape = 21, color = \u0026quot;black\u0026quot;)\rThere’s a lot that I want to change:\n\rI want the vizualization to be in “portrait” orientation. I’ll specify the dimensions in my code chunk options within the chunk header using fig.width=x and fig.height=y. (Where x and y are numbers, in inches.)\n\rThe x-axis should start at 0. I’m also going to expand on the right side so that there’s roughly equal padding on both sides of the dumbbells. For this I’ll use coord_cartesian().\n\rI want to customize my colors. For this I’ll specify the names and hex codes manually and use scale_fill_manual() to implement the names and values.\n\rI don’t want to use the default ggplot theme. Instead I’m going to use theme_minimal() to strip down everything to a lighter palette. Under theme() I’m also going to add additional customizations, namely the legend size, location, and orientation as well as the title, subtitle, and footnote formatting. I’m also going to remove the y-axis gridlines.\n\rTo make it very obvious that the 2020 data is projected data, I’m adding a text annotation (annotate(geom = \"text\")) and an arrow annotation (annotate(geom = \"curve\")).\n\rFinally, I will specify my title, subtitle, footnote, legend title, and axis labels using labs().\r\rSee the final code and visualization:\nmycolors \u0026lt;- c(\u0026quot;W_per\u0026quot; = \u0026quot;#fed2b7\u0026quot;, \u0026quot;B_per\u0026quot; = \u0026quot;#55160d\u0026quot;)\rggplot() +\rgeom_segment(data = shootings, mapping = aes(x=W_per, xend=B_per, y=Year, yend=Year), size = 1) +\rgeom_point(data = shootings_long, mapping = aes(x = fatal_per, y = Year, group = race, fill = race), size=5, shape = 21, color = \u0026quot;black\u0026quot;) +\r# Customize appearance\rcoord_cartesian(xlim = c(0,8)) +\rscale_fill_manual(values = mycolors,\rlabels = c(\u0026quot;Black\u0026quot;,\u0026quot;White\u0026quot;)) +\rscale_y_discrete(expand = c(0.1,0,0,1)) + # Expand margins on top and bottom of plot\rtheme_minimal() +\rtheme(legend.position = c(0.115,0.98),\rlegend.background = element_rect(fill = \u0026quot;white\u0026quot;, color = \u0026quot;white\u0026quot;),\rlegend.text = element_text(size = 10),\rlegend.title = element_text(size = 10),\rlegend.direction = \u0026quot;horizontal\u0026quot;,\rplot.title = element_text(face = \u0026quot;bold\u0026quot;, size = 15, hjust=0.2),\rplot.title.position = \u0026quot;plot\u0026quot;,\rplot.subtitle = element_text(hjust = 0.1),\rplot.caption = element_text(hjust = 0),\rpanel.grid.major.y = element_blank()) +\r# Arrow annotation to projected data\rannotate(geom = \u0026quot;curve\u0026quot;, size = 1, color = \u0026quot;black\u0026quot;,\rx = 6.5, y = 5.8, xend = 5.35, yend = 6.1, curvature = 0.7,\rarrow = arrow(length = unit(2.5, \u0026quot;mm\u0026quot;))) +\r# Text annotation to projected data\rannotate(geom = \u0026quot;text\u0026quot;, x = 6.5, y = 5.63,\rlabel = \u0026quot;projected based on\\ncurrent data\u0026quot;, color = \u0026quot;black\u0026quot;, size = 3.5, lineheight = 0.8, hjust = 0.5) + # Customize labels\rlabs(title = \u0026quot;Police Shooting Fatalities by Victim Race\u0026quot;,\rsubtitle = \u0026quot;Racial bias is evident in the police system. Black people \\nare disproportionately victimized and murdered by police.\u0026quot;,\rfill = \u0026quot;Race\u0026quot;,\ry = \u0026quot;Year\u0026quot;,\rx = \u0026quot;Number of Fatal Shootings\\n(per 1,000,000 people of indicated race)\u0026quot;,\rcaption = \u0026quot;*Data current as of June 8, 2020\u0026quot;)\r\r\r","date":1595030400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595100785,"objectID":"892b39ea3d0f274c419a191d76bd69d0","permalink":"/post/racial-bias-in-fatal-police-shootings/","publishdate":"2020-07-18T00:00:00Z","relpermalink":"/post/racial-bias-in-fatal-police-shootings/","section":"post","summary":"Presentation:\r\rHear me talk through the making of this visualization. All code featured in the presentation (and more!) can be found in the sections below.\r\r\rThe Data:\rDespite the frequency with which incidents of police brutality occurs in the US, an official centralized record of police violence does not exist.","tags":["data viz","how to"],"title":"Racial Bias in Fatal Police Shootings","type":"post"},{"authors":[],"categories":[],"content":"\rSelf isolation during the COVID19 pandemic lockdown has impacted us all differently. A lot of us are baking bread and raising sourdough starters. Some of us (ie. me) are becoming collectors of graffiti tags. In order to try to get myself moving during the lockdown I started going on long walks around Portland. I saw a “Penis Girl” tag and thought it was funny so I snapped a picture. Soon I began seeing Penis Girl (PG) tags all over the place. After three months of collecting I accrued over 100 unique tags. It’s gotten to the point that I can’t really focus on anything when I’m around town because I’m busy scanning for Penis Girl tags. Soon enough I was noticing patterns and learning about Penis Girl. This page is a summary of what I’ve found.\nPG Territory\rThe territory here is going to be heavily impacted by the fact that I have not been systemmatically collecting tags and thus there are going to be more tags in the areas that I frequent and I’ll miss tags in areas that I don’t explore. The initial pattern that I noticed was that PG tags were most densely concentrated in SE Portland, less dense in NE Portland, and almost none in North Portland. Then I started noticing more around the riverfront and bleeding into the west side more and more. There is a high concentration of tags on Steel Bridge though I have also found tags on Hawthorne Bridge and Burnside Bridge, just less concentrated.\n\rStyle\rMost of the PG tags are just “normal” text with a few distinguishing characteristics: the loop on the “P”, the “N” that almost looks like a “U” sometimes, and the curl on the “L”. They also always write the “e” as lowercase whereas all the other letters are uppercase. Some of the PG tags are in a bigger “bubble” text style. In this case the “R” has a face. This style is much less common (9 out of 123 PG tags are in the bubble style).\n\rRecurring Patterns: Is “Penis Girl” more than one person?\rPretty early on I started to notice that PG tags were more than just “Penis Girl”. There were recurring symbols accompanying the tag. I began to wonder if maybe Penis Girl is more than just one “Girl”. These tags vary from tag to tag. Could be some kind of signature? Below are the recurring symbols:\nThe “circle A” anarchy symbol\rATF\rPC\rAnd then I started seeing some “PC” tags that looked slightly different - the “C” had a little line on it. Was this PC the same PC? (I still don’t know.)\nSo I had been sitting with the idea that maybe Penis Girl was multiple people. And then I saw it: a “Penis Crew” tag. Penis CREW! The style was consistent with the PG tags so I was confident it was a PG tag. Moreover…PC stands for Penis Crew! It’s multiple people!!\n\r\rSuspected PG Tags\rAfter the George Floyd murder, tagging and graffiti started to increase (and the city’s drive to cover up/scrub away tags also increased) with the civil unrest. I noticed a couple of postings that…looked PG-like to me.\nThe characteristic “P” and the anarchy symbols…I think it’s PG/PC!\n\r\rOccassional Added Flair\rSometimes a PG tag is more than just “Penis Girl” or “PC” or whatever. Here are some spotted tags with a little something extra to them. Hard to pick a favorite.\n\rThe PG Copycat\rMore recently I have started spotting tags where the style is not consistent with the PG style. The “P” is wrong. The capitalization is wrong. They’re just wrong. Is there a copycat out there? Or is there a new member of the Penis Crew that is not conforming to style standards??\n\rWhat about Beanz?\rAs I mentioned above, there are very few PG tags in North Portland. There is, however a high density of Beanz tags in North Portland.\nIs there a territory war between PG and Beanz? When I saw this tag in North Portland I started to think that maybe (maybe) there was.\nSo far I’ve only spotted Beanz in North Portland, where there are almost no PG tags.\n\rConclusion\rIf you’re in Portland…keep an eye out for PG/PC (and Beanz)!!\n\r","date":1594166400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594166400,"objectID":"92e05c4d2aed7384088f3b0835ede810","permalink":"/project/the-penis-girl-observer/","publishdate":"2020-07-08T00:00:00Z","relpermalink":"/project/the-penis-girl-observer/","section":"project","summary":"Self isolation during the COVID19 pandemic lockdown has impacted us all differently. A lot of us are baking bread and raising sourdough starters. Some of us (ie. me) are becoming collectors of graffiti tags.","tags":[],"title":"The Penis Girl Observer","type":"project"},{"authors":[],"categories":[],"content":"\rThe Data: SIDER 4.1 Side Effect Resource\rFrom the website:\n\rSIDER contains information on marketed medicines and their recorded adverse drug reactions. The information is extracted from public documents and package inserts. The available information include side effect frequency, drug and side effect classifications as well as links to further information, for example drug–target relations.\n\r\rThe Visualization: What are the most common side effects of psychiatric medications?\r\rThe Details: How the Plot was Made\rThis plot was made using ggplot. It uses geom_bar with coord_theta to change the bar to a circle. The color scheme is the Futurama palette from the ggsci package. Here is a glimpse of the data:\n\r\rSTITCH\rDrug\rMedDra_code\rMedDra_term\rfreq_lb\rfreq_ub\r\r\r\rCID100000444\rbupropion\rC0004093\rAsthenia\r0.022\r0.164\r\rCID100000444\rbupropion\rC0009806\rConstipation\r0.096\r0.096\r\rCID100000444\rbupropion\rC0011991\rDiarrhoea\r0.052\r0.052\r\rCID100000444\rbupropion\rC0012833\rDizziness\r0.064\r0.064\r\rCID100000444\rbupropion\rC0015672\rFatigue\r0.050\r0.050\r\rCID100000444\rbupropion\rC0018681\rHeadache\r0.191\r0.290\r\rCID100000444\rbupropion\rC0027497\rNausea\r0.096\r0.096\r\rCID100000444\rbupropion\rC0042963\rVomiting\r0.046\r0.170\r\rCID100000444\rbupropion\rC0043352\rDry mouth\r0.150\r0.150\r\rCID100000444\rbupropion\rC0232462\rDecreased appetite\r0.051\r0.051\r\r\r\rAnd here is the ggplot code:\ntop12_psych %\u0026gt;% filter(freq_lb \u0026gt;= 0.1) %\u0026gt;% ggplot(mapping = aes(x = 1, y=freq_lb, fill = MedDra_term)) + geom_bar(stat = \u0026quot;identity\u0026quot;, position = \u0026quot;fill\u0026quot;, width = 0.1, color = \u0026quot;black\u0026quot;) +\rcoord_polar(theta=\u0026quot;y\u0026quot;) +\rfacet_wrap(~Drug, ncol=5) +\rxlim(c(0.9, 1.05)) +\rlabs(title = \u0026quot;What are the most common side effects of psychiatric medications?\u0026quot;,\rsubtitle = \u0026quot; \u0026quot;) +\rtheme_void() +\rtheme(legend.position = \u0026quot;top\u0026quot;,\rlegend.title = element_blank(),\rplot.title = element_text(hjust=0.5, size = 20),\rplot.margin = margin(t = 20, r = 0, b = 20, l = 0, unit = \u0026quot;pt\u0026quot;),\rplot.subtitle = element_text(size=1),\rstrip.background = element_rect(colour=\u0026quot;black\u0026quot;, fill=\u0026quot;grey90\u0026quot;),\rstrip.text = element_text(size = 10, vjust=1, hjust=0.5, margin=margin(3,0,3,0,\u0026quot;pt\u0026quot;))) +\rscale_fill_futurama()\r\r","date":1593043200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593120243,"objectID":"96d3aff256f4ff1a45d99ad4f4cc1508","permalink":"/post/side-effects-of-psychiatric-medications/","publishdate":"2020-06-25T00:00:00Z","relpermalink":"/post/side-effects-of-psychiatric-medications/","section":"post","summary":"The Data: SIDER 4.1 Side Effect Resource\rFrom the website:\n\rSIDER contains information on marketed medicines and their recorded adverse drug reactions. The information is extracted from public documents and package inserts.","tags":["data viz"],"title":"Side Effects of Psychiatric Medications","type":"post"},{"authors":[],"categories":[],"content":"\r\rRadar plots are also known as spider web or polar plots. These charts are useful for conveying information about multiple quantitative variables using multiple axes, arranged in a circle. In R it is technically possible to use ggplot2 to make these kinds of charts but the fmsb package allows for much easier and more readily customizable charts. (See fmsb’s CRAN page and RDocumentation page for more details.)\nThe Data: NeuroElectro Database of Electrical Properties of Brain Cells\rNeuroElectro is a great resource for electrophysiologists in neuroscience. This project extracts information about the electrophysiological properties of neurons from existing literature and integrates it into a centralized database. There are dozens of measurements documented for a large number of cell types in multiple species and preparations.\nCitation:\rNeuroElectro: a window to the world’s neuron electrophysiology data.\rFrontiers in Neuroinformatics, April 2014\rTripathy SJ, Savitskaya J, Burton SD, Urban NN, and Gerkin RC\rDescription: A methods paper outlining the text-mining and manual curation methodology used to construct the NeuroElectro resource.\n\rRepresentaiton Description\rFor my visualization I am focusing on in vitro patch clamp data in mouse tissue, because that’s what I work with in the lab. I decided to feature only 6 cell types which were more or less randomly chosen using my personal bias. I chose to focus on only 5 physiological properties even though there were many more in the dataset. Each radar plot here represents a single cell type. My intent is to showcase electrical properties of these cells for quick comparison. This is a lot of data in a small space. The same data could be conveyed using five bar graphs instead (in fact, if you were interested in the absolute measurements, bar graphs would be better.\nFor the fmsb package I need to include the maximum and minimum data for each variable so that it knows how to scale its axes. Here is the data that I am working with:\n\r\r\rInput Resistance\r\rResting Membrane Potential*\r\rCapacitance\r\rRheobase\r\rSpike Amplitude\r\r\r\r\r\rMax\r\r221.53\r\r81.64\r\r614.61\r\r1300.00\r\r86.42\r\r\r\rMin\r\r0.00\r\r0.00\r\r0.00\r\r0.00\r\r0.00\r\r\rNeurons\r\r\r\rNeocortex pyramidal cell layer 5-6\r\r162.13\r\r70.35\r\r89.23\r\r166.11\r\r79.33\r\r\r\rNeocortex basket cell\r\r158.96\r\r67.41\r\r51.31\r\r281.98\r\r61.22\r\r\r\rHippocampus CA1 pyramidal cell\r\r155.91\r\r66.55\r\r94.55\r\r75.60\r\r86.42\r\r\r\rNeostriatum medium spiny neuron\r\r121.20\r\r81.64\r\r80.50\r\r263.62\r\r81.59\r\r\r\rCerebellum Purkinje cell\r\r125.31\r\r62.02\r\r614.61\r\r1300.00\r\r81.32\r\r\r\rHippocampus CA3 pyramidal cell\r\r221.53\r\r67.28\r\r208.54\r\r92.00\r\r82.25\r\r\r\r\r\r* Note: Resting membrane potential is shown positive, but is in fact negative. This is because the radar plot struggles with negative values.\r\r\r\r\r\rHow to interpret\rAs I mentioned above, each radar plot represents a single cell type. Here, color serves no purpose other to indicate that each plot is a unique cell type. Each of the five axes represents a different measurement and has a different scale. The minimum is 0 in all cases. The maximum is indicated y the number at the apex of the axis. The lines indicate the percent of maximum from 0% to 100%, with each segment representing 25% of the maximum. This is indicated on the vertical axis, the only one with labels at each segment. It’s easy to see, for example, that cerebellar Purkinje cells have significantly higher cell capacitance and rheobase compared to other cell types. We can also see that all of this cell types have similar restine membrane potentials.\n\rPresentation Tips\rWhile fmsb will allow you to create as many axes as you want, I wouldn’t do more than 6. Beyond 6 things start to get difficult to interpret. Additionally, it is easiest to understand radar plots when each of the axes have the same scale (which mine do not). If the axes do have different scales, try to be as explicit as possible with labels. Unfortunately only the “center” axis (the vertical one) can show labels at each segment break. While the fmsb package contains many variables that can be modified to customize the plot, I found that it lacks in specifying text. For example, there is no way to adjust the text justification (left, center, right) of only one text element, you can only do it for all or none. You also cannot manually move text labels; this results in a lot of overlapping text, requiring you to get creative.\n\rVariations and Alternatives\rThere is a lot of hate for radar plots in the data viz world. In most cases, a series of bar graphs or a parallel coordinate plot conveys the data in a more easily interpretable way, albeit not as cool looking.\n\rThe Radar Plot\rThe most basic radar plot\rBefore getting too fancy, what does fmsb do with basically no embelishments? For this, we’ll use just one cell type. I decided to use Neocortex pyramidal cell (layer 5/6) data for this example. The top two rows of the data frame represent the axis max and min values. The next row(s) are the actual data. It should look like this:\n## # A tibble: 3 x 5\r## rin rmp cap rheo apamp\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 222. 81.6 615. 1300 86.4\r## 2 0 0 0 0 0 ## 3 162. 70.3 89.2 166. 79.3\rNow, feed it into fmsb::radarchart():\npar(mar=c(0,0,1.2,0)+0.1) # Set the margins\rradarchart(cortexpyramidal, title=\u0026quot;Neocortex pyramidal cell layer 5-6\u0026quot;) # Make the default radar plot\r\rThe embelished radar plot\r\rI used par() to customize the output. You can do a lot with par. Find helpful documentation here.\n\rThere’s a lot you can do with fmsb::radarchart()! For full documentation, go here.\r\r# Define colors for border and shading\rbordercol=colormap(colormap=colormaps$portland, nshades=6, alpha=1)\rshadingcol=colormap(colormap=colormaps$portland, nshades=6, alpha=0.3)\r# Set the titles\rtitles \u0026lt;- as.character(neuron.data$NeuronName[3:8])\r# Split the graphic into 6 frames\rpar(mar=c(0.1,0.5,0.5,0.1)+0.1) # Define margins\rpar(mfrow=c(2,3)) # 2 rows, 3 column layout\r# Loop through each subplot to build the 6 panels\rneuron.data.input \u0026lt;- neuron.data %\u0026gt;% select(-NeuronName) # Get rid of the \u0026quot;NeuronName\u0026quot; column\rfor(i in 1:6){\r# Build the radarChart\rradarchart(neuron.data.input[c(1,2,i+2),], axistype=3, # Build polygon\rpcol=bordercol[i] , pfcol=shadingcol[i] , plwd=2, plty=1 , # Define grid properties\rcglcol=\u0026quot;grey\u0026quot;, cglty=1, axislabcol=\u0026quot;black\u0026quot;, # The weird spaces below are because you cannot define text alignment for individual compondents of the graph and text was overlapping.\rpaxislabels = c(NA,\u0026quot;\\n-80mV \u0026quot;,\u0026quot;615pF\u0026quot;,\u0026quot;1300pA\u0026quot;,\u0026quot;\\n 85mV\u0026quot;), cglwd=0.8, caxislabels = c(\u0026quot;0%\u0026quot;,\u0026quot;25%\u0026quot;,\u0026quot;50%\u0026quot;,\u0026quot;75%\u0026quot;,\u0026quot;220MOhm\u0026quot;),\r# Add titles\rtitle=titles[i],\r# Customize labels\rvlabels=c(expression(\u0026#39;R\u0026#39;[\u0026#39;in\u0026#39;]), expression(\u0026quot;V\u0026quot;[\u0026quot;m\u0026quot;]), \u0026quot;Capacitance\u0026quot;, \u0026quot;Rheobase\u0026quot;, \u0026quot;Spike\\nAmp\u0026quot;),\rvlcex=1.2, palcex=0.9, calcex=0.9,\r)\r}\r\r\r","date":1591142400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591142400,"objectID":"b094dc92d25a5768f047e122e8f1d480","permalink":"/project/radar-diagrams/","publishdate":"2020-06-03T00:00:00Z","relpermalink":"/project/radar-diagrams/","section":"project","summary":"Radar plots are also known as spider web or polar plots. These charts are useful for conveying information about multiple quantitative variables using multiple axes, arranged in a circle.","tags":["how to","data viz"],"title":"Radar Diagrams","type":"project"},{"authors":[],"categories":[],"content":"\rThe Data: Portland, OR 2010 Census Data\rPortland provides open access maps and GIS data as well as census data. Here I chose to focus on data from each Portland neighborhood describing the number of individuals who either own their housing versus rent their housing. For the map I am also using data describing the layout of the city neighborhoods and the Willamette River.\n\rThe Visualization: Percent of Housing Units that are Owned, by Neighborhood\rHere I am using color to indicate the percent of housing units that are owned in a given neighborhood. With this color scale, red indicates that more units are owned than rented and blue indicates that more units are rented than owned. White indicates that an equal number of units are owned and rented. As you can see, in the neighborhoods surroundng the downtown area/city center, more units are rented than owned but in the majority of other neighborhoods the opposite is true.\n\rThe Details: How the Plot was Made\rThis plot was made using ggplot2 with geom_sf from the sf package. The data is organized into shp/shapefile folders and imported using st_read() (also part of sf). This results in an information-rich dataframe that makes these kinds of plots easy to create.\nboundaries_with_pop %\u0026gt;% ggplot() + geom_sf(aes(fill=`Percent Owned`), color = \u0026quot;black\u0026quot;) + # plot the neighborhoods\rscale_fill_gradient2(midpoint = 50,\rlow=\u0026quot;blue\u0026quot;, mid=\u0026quot;white\u0026quot;, high=\u0026quot;red\u0026quot;) + # specify the gradient color scale\rgeom_sf(data=river_boundaries, fill=\u0026quot;blue\u0026quot;, color = \u0026quot;transparent\u0026quot;) + # plot the river\rlabs(title = \u0026quot;Percent of Housing Units that are Owned (Not Rented), \\nby Neighborhood\u0026quot;,\rfill = \u0026quot;% Owned\u0026quot;) +\rtheme_minimal()\r\r","date":1591056000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591116259,"objectID":"817ff1576ee00007627ce5afd7327cb0","permalink":"/post/home-ownership-in-portland-or/","publishdate":"2020-06-02T00:00:00Z","relpermalink":"/post/home-ownership-in-portland-or/","section":"post","summary":"The Data: Portland, OR 2010 Census Data\rPortland provides open access maps and GIS data as well as census data. Here I chose to focus on data from each Portland neighborhood describing the number of individuals who either own their housing versus rent their housing.","tags":["data viz"],"title":"Home Ownership in Portland, OR","type":"post"},{"authors":[],"categories":[],"content":"\r\r\r\r\r\r\rAlso known as “radial network diagrams”, chord diagrams are useful for representing connections between groups (“nodes”). The nodes are circularly arranged and relationships are represented using “chords” connecting two nodes. The chords can carry directional relationships or non-directional relationships. This is a very visually pleasing way to represent relationships and is a powerful method of visualizing large datasets. There is, however, a steep learning curve when it comes to creating chord diagrams. Below I will outline two methods, first I will use the circlize package to create a static diagram and then I will use the chorddiag package to create an interactive diagram. For more information on how to build chord diagrams using circlize, see Circular Visualization in R by Zuguang Gu (the creator of the circlize package). The documentation for circlize can be found here. There is some useful information on the R Graph Gallery chord diagram page (more here and here). For more information on the chorddiag package, you can find the documentation here. For an explanation of all of the variables you can customize, see additional documentation here.\nThe Data: How Couples Meet and Stay Together\rThe datset is from Stanford’s How Couples Meet and Stay Together research project. The dataset contains responses from 4,000 individuals and describes the relationships in their lives. The researchers then followed up on the respondents over several years to track their relationships over time. There are 300+ variables in this dataset. Here I focused on the religious identities of the couples surveyed.\nA glimpse of the data: What are the religious compositions of partners?\r\r\rRespondent\r\rPartner\r\rCount\r\r\r\r\r\rBaptist\r\rBaptist\r\r224\r\r\r\rBaptist\r\rBuddhist\r\r1\r\r\r\rBaptist\r\rCatholic\r\r112\r\r\r\rBaptist\r\rJewish\r\r2\r\r\r\rBaptist\r\rMormon\r\r7\r\r\r\rBaptist\r\rMuslim\r\r3\r\r\r\rBaptist\r\rNone\r\r72\r\r\r\rBaptist\r\rOther\r\r9\r\r\r\rBaptist\r\rOther Christian\r\r67\r\r\r\rBaptist\r\rPentecostal\r\r26\r\r\r\rBaptist\r\rProtestant\r\r147\r\r\r\rBuddhist\r\rCatholic\r\r5\r\r\r\rBuddhist\r\rNone\r\r7\r\r\r\rBuddhist\r\rOther\r\r1\r\r\r\rBuddhist\r\rOther Christian\r\r1\r\r\r\rBuddhist\r\rProtestant\r\r4\r\r\r\rCatholic\r\rCatholic\r\r415\r\r\r\rCatholic\r\rEastern Orthodox\r\r5\r\r\r\rCatholic\r\rHindu\r\r1\r\r\r\rCatholic\r\rJewish\r\r40\r\r\r\r\r\r\rRepresentation Description\rOne of the variables in the dataset is the religion of the responent and their partner when they were 16. Each node in my chord diagram represents a religion. Chords connecting the nodes illustrate the religion of the respondent on one end of the chord and their partner on the other end of the chord. In many cases the respondent had the same religious identity as their partner, in which case the chord loops back on itself and looks more like a bump. The thickness of the chord corresponds to the number of partners of that particular religous identity composition. Each node is assigned a color to aid in interpretation. The color of a chord corresponds to the node from which the chord stems from. Transparency was used to aid in the perception of overlapping chords.\n\rHow to interpret\rChord diagrams are best used to communicate broad concepts rather than specifics. From this representation it becomes immediately apparent that a very common religious pairing occurs between Catholics and Protestants. It is also common for partners to share the same religious identity. A side effect of this representation is that we can learn something about the religious composition of the participants that were surveyed; most respondents were Protestant, Baptist, or Catholic.\n\rPresentation Tips\rMany aspects of the diagram can be customized if you’re using the circlize package. The package creator has an entire book on the topic available online: Circular Visualization in R. You can indicate directionality of the chords using arrowheads, which I disabled in my diagrams. You can also change the appearance of within-node chords such that they look like little bumps instead of a chord turned in on itself, which I find easier to understand. Depending on the level of detail that you are trying to convey with your diagram, you might opt to eliminate smaller chords by setting a threshold for chord thickness.\n\rVariations and Alternatives\rAnother way to represent relationships between groups is through a Sankey or Alluvial diagram. These are very similar to chord diagrams except that the relationship is conveyed through a line connecting two columns and can thus show a larger number of relationships due to the linear layout. Alternatively, network diagrams and arc diagrams can also be useful for communicating relationships.\nThe Chord Diagrams\rStatic Chord Diagram Using circlize\r# Define colors\rgridcolors \u0026lt;- c(Baptist = \u0026quot;#F8766D\u0026quot;,\rBuddhist = \u0026quot;#E18A00\u0026quot;,\rCatholic = \u0026quot;#BE9C00\u0026quot;,\r`Eastern Orthodox` = \u0026quot;#8CAB00\u0026quot;,\rHindu = \u0026quot;#24B700\u0026quot;,\rJewish = \u0026quot;#00BE70\u0026quot;,\rMormon = \u0026quot;#00C1AB\u0026quot;,\rMuslim = \u0026quot;#00BBDA\u0026quot;,\rNone = \u0026quot;#00ACFC\u0026quot;,\rOther = \u0026quot;#8B93FF\u0026quot;,\r`Other Christian` = \u0026quot;#D575FE\u0026quot;,\rPentecostal = \u0026quot;#F962DD\u0026quot;,\rProtestant = \u0026quot;#FF65AC\u0026quot;)\rchordcolors \u0026lt;- religion_16_summary %\u0026gt;% mutate(Color = case_when(Respondent == \u0026quot;Baptist\u0026quot; ~ \u0026quot;#F8766D\u0026quot;,\rRespondent == \u0026quot;Buddhist\u0026quot; ~ \u0026quot;#E18A00\u0026quot;,\rRespondent == \u0026quot;Catholic\u0026quot; ~ \u0026quot;#BE9C00\u0026quot;,\rRespondent == \u0026quot;Eastern Orthodox\u0026quot; ~ \u0026quot;#8CAB00\u0026quot;,\rRespondent == \u0026quot;Hindu\u0026quot; ~ \u0026quot;#24B700\u0026quot;,\rRespondent == \u0026quot;Jewish\u0026quot; ~ \u0026quot;#00BE70\u0026quot;,\rRespondent == \u0026quot;Mormon\u0026quot; ~ \u0026quot;#00C1AB\u0026quot;,\rRespondent == \u0026quot;Muslim\u0026quot; ~ \u0026quot;#00BBDA\u0026quot;,\rRespondent == \u0026quot;None\u0026quot; ~ \u0026quot;#00ACFC\u0026quot;,\rRespondent == \u0026quot;Other\u0026quot; ~ \u0026quot;#8B93FF\u0026quot;,\rRespondent == \u0026quot;Other Christian\u0026quot; ~ \u0026quot;#D575FE\u0026quot;,\rRespondent == \u0026quot;Pentecostal\u0026quot; ~ \u0026quot;#F962DD\u0026quot;,\rRespondent == \u0026quot;Protestant\u0026quot; ~ \u0026quot;#FF65AC\u0026quot;))\rchordcolors \u0026lt;- chordcolors$Color %\u0026gt;% unlist()\r# Create the chord diagram\rcircos.clear()\rpar(mar = c(0, 0, 0.5, 0)) # left, right, top, bottom: add margin around circle\rcircos.par(cell.padding = c(0, 0, 0, 0),\rgap.degree = 1,\rcanvas.ylim = c(-0.6, 0.8), # change the y limits of the canvas\rcanvas.xlim = c(-1.1, 1.1)) # change the x limits of the canvas\rchordDiagram(religion_16_summary,\rtransparency = 0.5, grid.col = gridcolors, link.lwd = 0.5, # border line width\rlink.lty = 1, # border line type\rlink.border = chordcolors, # border line color\rlink.sort = TRUE, link.decreasing = TRUE, # Control the positioning of the sector links to minimize crossings\rself.link = 1, # Make self-links humps, not chords ( = 2 for chords)\rannotationTrack = \u0026quot;grid\u0026quot;, # We\u0026#39;ll plot the labels later\rannotationTrackHeight = 0.02, # Height for the annotation \u0026quot;grid\u0026quot;\rpreAllocateTracks = 1, # Pre allocate a track and later the sector labels will be added\rdirectional = FALSE, # There is no directionality to the connections\rorder = religion_16_summary$Respondent) # Order according the the respondent first, partner second\r# Since each respondent is a sector, we need to use `draw.sector` to add annotation grids for regions which go across several religions\rfirst = tapply(religion_16_summary$Respondent, religion_16_summary$Partner, function(x) x[1])\rlast = tapply(religion_16_summary$Respondent, religion_16_summary$Partner, function(x) x[length(x)])\rfor(i in seq_along(first)) {\rstart.degree = get.cell.meta.data(\u0026quot;cell.start.degree\u0026quot;, sector.index = first[i], track.index = 1)\rend.degree = get.cell.meta.data(\u0026quot;cell.end.degree\u0026quot;, sector.index = last[i], track.index = 1)\rrou1 = get.cell.meta.data(\u0026quot;cell.bottom.radius\u0026quot;, sector.index = first[i], track.index = 1)\rrou2 = get.cell.meta.data(\u0026quot;cell.top.radius\u0026quot;, sector.index = last[i], track.index = 1)\rdraw.sector(start.degree, end.degree, rou1, rou2, border = NA, col = \u0026quot;white\u0026quot;)\r}\r# Since default text facing in `chordDiagram` is fixed, we need to manually add text in track 1\rfor(si in get.all.sector.index()) {\rxlim = get.cell.meta.data(\u0026quot;xlim\u0026quot;, sector.index = si, track.index = 1)\rylim = get.cell.meta.data(\u0026quot;ylim\u0026quot;, sector.index = si, track.index = 1)\rcircos.text(mean(xlim), ylim[1], si, facing = \u0026quot;clockwise\u0026quot;, adj = c(0, 0.5),\rniceFacing = TRUE, cex = 1.25, col = \u0026quot;black\u0026quot;, sector.index = si, track.index = 1)\r}\r\rInteractive Chord Diagram Using chorddiag\rFor chorddiag the data needs to be organized as a matrix, but the subsequent coding for the diagram is much simpler.\n\r\r\rBaptist\r\rBuddhist\r\rCatholic\r\rEastern Orthodox\r\rHindu\r\rJewish\r\rMormon\r\rMuslim\r\rNone\r\rOther\r\rOther Christian\r\rPentecostal\r\rProtestant\r\r\r\r\r\rBaptist\r\r224\r\r0\r\r56\r\r0\r\r0\r\r0\r\r3\r\r1\r\r33\r\r5\r\r38\r\r14\r\r77\r\r\r\rBuddhist\r\r1\r\r0\r\r2\r\r0\r\r0\r\r0\r\r0\r\r0\r\r3\r\r1\r\r1\r\r0\r\r1\r\r\r\rCatholic\r\r56\r\r3\r\r415\r\r2\r\r1\r\r19\r\r9\r\r0\r\r81\r\r13\r\r63\r\r8\r\r172\r\r\r\rEastern Orthodox\r\r0\r\r0\r\r3\r\r3\r\r1\r\r1\r\r0\r\r0\r\r0\r\r1\r\r2\r\r0\r\r0\r\r\r\rHindu\r\r0\r\r0\r\r0\r\r0\r\r5\r\r0\r\r0\r\r1\r\r0\r\r0\r\r0\r\r0\r\r1\r\r\r\rJewish\r\r2\r\r0\r\r21\r\r0\r\r2\r\r32\r\r1\r\r0\r\r9\r\r2\r\r1\r\r0\r\r13\r\r\r\rMormon\r\r4\r\r0\r\r6\r\r0\r\r0\r\r0\r\r38\r\r0\r\r4\r\r0\r\r3\r\r0\r\r9\r\r\r\rMuslim\r\r2\r\r0\r\r1\r\r0\r\r0\r\r0\r\r0\r\r3\r\r3\r\r0\r\r1\r\r1\r\r1\r\r\r\rNone\r\r39\r\r4\r\r92\r\r3\r\r0\r\r5\r\r3\r\r0\r\r134\r\r20\r\r50\r\r10\r\r75\r\r\r\rOther\r\r4\r\r0\r\r7\r\r0\r\r0\r\r1\r\r1\r\r0\r\r8\r\r3\r\r2\r\r1\r\r7\r\r\r\rOther Christian\r\r29\r\r0\r\r52\r\r1\r\r1\r\r4\r\r2\r\r1\r\r42\r\r8\r\r106\r\r9\r\r47\r\r\r\rPentecostal\r\r12\r\r0\r\r3\r\r0\r\r1\r\r0\r\r0\r\r0\r\r7\r\r0\r\r9\r\r17\r\r8\r\r\r\rProtestant\r\r70\r\r3\r\r166\r\r4\r\r0\r\r15\r\r5\r\r0\r\r46\r\r9\r\r39\r\r14\r\r319\r\r\r\r\r# Define colors\rdiagcolors \u0026lt;- c(\u0026quot;#F8766D\u0026quot;, \u0026quot;#E18A00\u0026quot;, \u0026quot;#BE9C00\u0026quot;, \u0026quot;#8CAB00\u0026quot;, \u0026quot;#24B700\u0026quot;, \u0026quot;#00BE70\u0026quot;, \u0026quot;#00C1AB\u0026quot;, \u0026quot;#00BBDA\u0026quot;, \u0026quot;#00ACFC\u0026quot;, \u0026quot;#8B93FF\u0026quot;, \u0026quot;#D575FE\u0026quot;, \u0026quot;#F962DD\u0026quot;, \u0026quot;#FF65AC\u0026quot;)\r# Create the chord diagram\rchorddiag(religionmatrix, type = \u0026quot;directional\u0026quot;,\rgroupColors = diagcolors, groupnamePadding = 5, groupnameFontsize = 18,\rchordedgeColor = \u0026quot;\u0026quot;,\rshowTicks = FALSE,\rshowZeroTooltips = FALSE)\r\r{\"x\":{\"matrix\":[[224,0,56,0,0,0,3,1,33,5,38,14,77],[1,0,2,0,0,0,0,0,3,1,1,0,1],[56,3,415,2,1,19,9,0,81,13,63,8,172],[0,0,3,3,1,1,0,0,0,1,2,0,0],[0,0,0,0,5,0,0,1,0,0,0,0,1],[2,0,21,0,2,32,1,0,9,2,1,0,13],[4,0,6,0,0,0,38,0,4,0,3,0,9],[2,0,1,0,0,0,0,3,3,0,1,1,1],[39,4,92,3,0,5,3,0,134,20,50,10,75],[4,0,7,0,0,1,1,0,8,3,2,1,7],[29,0,52,1,1,4,2,1,42,8,106,9,47],[12,0,3,0,1,0,0,0,7,0,9,17,8],[70,3,166,4,0,15,5,0,46,9,39,14,319]],\"options\":{\"type\":\"directional\",\"width\":null,\"height\":null,\"margin\":100,\"showGroupnames\":true,\"groupNames\":[\"Baptist\",\"Buddhist\",\"Catholic\",\"Eastern Orthodox\",\"Hindu\",\"Jewish\",\"Mormon\",\"Muslim\",\"None\",\"Other\",\"Other Christian\",\"Pentecostal\",\"Protestant\"],\"groupColors\":[\"#F8766D\",\"#E18A00\",\"#BE9C00\",\"#8CAB00\",\"#24B700\",\"#00BE70\",\"#00C1AB\",\"#00BBDA\",\"#00ACFC\",\"#8B93FF\",\"#D575FE\",\"#F962DD\",\"#FF65AC\"],\"groupThickness\":0.1,\"groupPadding\":0.0349065850398866,\"groupnamePadding\":[5,5,5,5,5,5,5,5,5,5,5,5,5],\"groupnameFontsize\":18,\"groupedgeColor\":null,\"chordedgeColor\":\"\",\"categoryNames\":null,\"categorynamePadding\":100,\"categorynameFontsize\":28,\"showTicks\":false,\"tickInterval\":10,\"ticklabelFontsize\":10,\"fadeLevel\":0.1,\"showTooltips\":true,\"showZeroTooltips\":false,\"tooltipNames\":[\"Baptist\",\"Buddhist\",\"Catholic\",\"Eastern Orthodox\",\"Hindu\",\"Jewish\",\"Mormon\",\"Muslim\",\"None\",\"Other\",\"Other Christian\",\"Pentecostal\",\"Protestant\"],\"tooltipFontsize\":12,\"tooltipUnit\":\"\",\"tooltipGroupConnector\":\" \u0026#x25B6; \",\"precision\":\"null\",\"clickAction\":null,\"clickGroupAction\":null}},\"evals\":[],\"jsHooks\":[]}\r\r\r\r","date":1590624000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590624000,"objectID":"bf27542347ca2e36e913e1366f83fc5c","permalink":"/project/chord-diagrams/","publishdate":"2020-05-28T00:00:00Z","relpermalink":"/project/chord-diagrams/","section":"project","summary":"Also known as “radial network diagrams”, chord diagrams are useful for representing connections between groups (“nodes”). The nodes are circularly arranged and relationships are represented using “chords” connecting two nodes.","tags":["data viz","how to"],"title":"Chord Diagrams","type":"project"},{"authors":[],"categories":[],"content":"\rTo help us visualize 3D functions, there are methods to bring the function down a dimension to give you a little taste of how it’s behaving. One of these methods is gradient fields. Gradient fields represent the maximum rate of change of a 3D function at a point in space. You know those maps of mountain ranges that have all the concentric curves representing altitude? It’s kind of like that, but with some extra info. (Those maps are a little closer to contour maps, which are another method for visualizing three dimensional functions in two dimensions.) Calculating a vector field involves taking a given three dimensional function and calculating “grad f” – yielding a general vector equation. Each coordinate you plug into grad f will give you a little vector describing the behavior of that point. Calculating a bunch of these vectors gives you a pretty vector field.\n","date":1590624000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590624000,"objectID":"236969060619b808bfa76f37d8b8f400","permalink":"/project/gradient-field/","publishdate":"2020-05-28T00:00:00Z","relpermalink":"/project/gradient-field/","section":"project","summary":"To help us visualize 3D functions, there are methods to bring the function down a dimension to give you a little taste of how it’s behaving. One of these methods is gradient fields.","tags":["science around us"],"title":"Gradient Field","type":"project"},{"authors":[],"categories":[],"content":"\rHave you ever wondered how we make these bright and beautiful images of proteins in the brain? It’s done using a technique called immunohistochemistry, which we also call IHC or immuno. The technique used antibodies to label proteins in fixed tissue.\n\r\rFirst, I preserve the brain using formaldehyde. This “fixes” the proteins in place. I then embed the brain in a block of agar and slice it into thin sections. There are several ways to do this but I use a vibrating razor blade.\n\rNow I split up my sections into different wells. Each well will contain a different combination of antibodies so that I can label different proteins.\n\r\rThe combination of antibodies matters. I need a primary antibody, which recognizes my protein of interest, and a secondary antibody which latches on to the primary antibody. The secondary antibody contains a fluorescent “tag”. This tag is what the microscope sees.\n\r\rIn order to look at multiple proteins in the same tissue section, I can use other fluorescent tags that emit different wavelengths of light.\n\r\rYou’ll notice that all of my images only have 3-4 different colors in them. That’s because we need to be able to separate the fluorescent tags. If the emitted light of two tags have wavelengths that are too close together we cannot tell them apart.\n\r\rOkay, so I have my antibodies where I want them. Now I mount the tissue sections onto microscope slides and they’re ready to be imaged on the microscope!\n\r\rThe microscope images one “channel” of fluorescence at a time. In this case I used 4 different antibodies for 4 different proteins. (The numbers in the parentheses are the wavelengths of the emitted light from the fluorescent tag.)\n\r\rNow I can overlay the images on one another to get that beautiful merged image. The coloring in the merged image doesn’t necessarily correspond to the fluorescent tags I used. I can use any color combination I want. These four images are all the same micrograph.\n\r\rAnd that’s it! Each step has additional intricacies that I glossed over for the sake of simplicity. There are also a number of different microscopy methods that could be used. These images were all taken using a Zeiss Axio Imager equipped with an apotome system.\nWanna talk about it? Hit up my Twitter thread.\n","date":1590624000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590624000,"objectID":"6a86b9c250068a7ca0c3b09ff20ec908","permalink":"/project/immunohistochemistry/","publishdate":"2020-05-28T00:00:00Z","relpermalink":"/project/immunohistochemistry/","section":"project","summary":"Have you ever wondered how we make these bright and beautiful images of proteins in the brain? It’s done using a technique called immunohistochemistry, which we also call IHC or immuno.","tags":["sci comm"],"title":"Immunohistochemistry: Adding Color to the Brain","type":"project"},{"authors":[],"categories":[],"content":"\rThe whole world is on lockdown due to the SARS-CoV-2 pandemic that began in 2019 and continues to grow today. As lockdown orders have been issued, the way people move about their hometowns have changed. More people are working from home and only grocery shopping every couple of weeks. Apple has made available trends in mobility in major cities and countries as measured through usage of the Apple Maps application. Data is available from January 2020 onwards and can be accessed through the covdata R package. Below is an interactive widget to view the mobility trends for individual countries. (The app can also be found here.)\n\r","date":1590624000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590688393,"objectID":"0641f1d4010e80627bfdff79a9d1bd58","permalink":"/post/mobility-during-covid19/","publishdate":"2020-05-28T00:00:00Z","relpermalink":"/post/mobility-during-covid19/","section":"post","summary":"The whole world is on lockdown due to the SARS-CoV-2 pandemic that began in 2019 and continues to grow today. As lockdown orders have been issued, the way people move about their hometowns have changed.","tags":["interactive"],"title":"Mobility During COVID19","type":"post"},{"authors":[],"categories":[],"content":"\rThe cortex of a human brain is complicated. It’s broken into 6 layers, of which several are broken down further into sublayers. Layer 4 receives primary input from lower brain areas. The other areas are all talking to each other or shipping information out or receiving information from higher areas. Without all of these feedback/feedforward connections we would be left unable to make inferences about the world. Let’s say you were looking at a nice, round penny. Primary visual cortex (V1), located at the back of the forebrain, is shipping information up the assembly line about the orientation of each the coins edges. A higher area receives that information and sends a query back down to V1: “circle?” V1 shouts back in agreement, the prediction checks out. Right now the coin is still just an abstract shape - what is it’s significance? Let’s send it up to a higher area. This new area scans for clues and sends back down a new query: “penny?” V1 checks it out…yes! Now we send the information all around the brain to pull out associations and understand the context and form new commands. The information will probably make a stop in the prefrontal cortex to ask what we’re doing with the penny. It might also stop in motor cortex to coordinate movements to interact with the coin. This is all a gross oversimplification of Predictive Coding. The basic idea is that through all of these interconnected brain regions communicating with each other, we are able to parse out information about the world that we would otherwise be blind to if limited to just one processing region.\n","date":1589155200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589155200,"objectID":"74bddfe185561e89d187789d7777f1ec","permalink":"/project/cortical-connections/","publishdate":"2020-05-11T00:00:00Z","relpermalink":"/project/cortical-connections/","section":"project","summary":"The cortex of a human brain is complicated. It’s broken into 6 layers, of which several are broken down further into sublayers. Layer 4 receives primary input from lower brain areas.","tags":["science around us"],"title":"Cortical Connections","type":"project"},{"authors":[],"categories":[],"content":"\rA and B are two separate events, but not necessarily completely separate. Sometimes A and B can happen together. The probability of that occurring is given by \\(P(A \\cap B)\\): the joint probability. \\(P(A \\cap B)\\) can be a pain to calculate; it’s easiest to deduce visually, but that isn’t always possible. However, in the special case in which A and B are known to be mutually exclusive (that is, they never occur together), you know that the joint probability is automatically zero: knowing something about A says nothing about B.\nOne roundabout way of finding the joint probability is by multiplying the probability of B occurring by the probability A occurring when you already know that B has occurred (the conditional probability). Not very direct, but really informative about the world. It’s hard to predict the probability of just one event occurring, but if you know that its “joint” event already occurred, you know so much more.\n","date":1589155200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589155200,"objectID":"1ddeee52bf19098b646710ef423128c8","permalink":"/project/joint-probability/","publishdate":"2020-05-11T00:00:00Z","relpermalink":"/project/joint-probability/","section":"project","summary":"A and B are two separate events, but not necessarily completely separate. Sometimes A and B can happen together. The probability of that occurring is given by \\(P(A \\cap B)\\): the joint probability.","tags":["science around us"],"title":"Joint Probability","type":"project"},{"authors":[],"categories":[],"content":"\rMath is everywhere. Here, the wilted shrub forms an arc. Draw a triangle with the tangent line as the hypotenuse, \\(\\Delta x\\) as the base, and \\(\\Delta y\\) as the height. The length of the hypotenuse represents an approximation for the length of that portion of the curve. By adding up all of those small hypotenuses, you can calculate the area of the whole arc. Tangent lines represent a linear approximation for a curve, but only near the point where it hits the curve. Even two tangent lines that touch the curve at fairly close points diverge to an extreme extent as you move away from the points around which they’re centered. It’s only through the use of infinite tangent lines that we can accurately measure an arc in this way.\n","date":1588896000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588896000,"objectID":"0225f7ba20764499423698c718d21e46","permalink":"/project/arc-length/","publishdate":"2020-05-08T00:00:00Z","relpermalink":"/project/arc-length/","section":"project","summary":"Math is everywhere. Here, the wilted shrub forms an arc. Draw a triangle with the tangent line as the hypotenuse, \\(\\Delta x\\) as the base, and \\(\\Delta y\\) as the height.","tags":["science around us"],"title":"Arc Length","type":"project"},{"authors":[],"categories":[],"content":"\rFinding the area under one curve involves taking the integral of that curve over the interval of interest: \\(\\int_{a}^{b}{f(x)dx}\\).\nIf you want to find the area between two curves (\\(f(x)\\) and \\(g(x)\\)), you start by taking the integral of the top curve and subtract from that the un-wanted area - the area under the bottom curve. One more step: find where the two curves intersect. That will give you the bounds to use for the integral (\\(a\\) and \\(b\\)).\n","date":1588896000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588896000,"objectID":"b6fe3031a8856dccbb710141666af3f0","permalink":"/project/area/","publishdate":"2020-05-08T00:00:00Z","relpermalink":"/project/area/","section":"project","summary":"Finding the area under one curve involves taking the integral of that curve over the interval of interest: \\(\\int_{a}^{b}{f(x)dx}\\).\nIf you want to find the area between two curves (\\(f(x)\\) and \\(g(x)\\)), you start by taking the integral of the top curve and subtract from that the un-wanted area - the area under the bottom curve.","tags":["science around us"],"title":"Area","type":"project"},{"authors":[],"categories":[],"content":"\rThe back of your retina is lined with light-sensitive rods and cones. These photoreceptors catch the light and send the light information to bipolar cells, who then send it to retinal ganglion cells (RGCs). It’s here, at the RGCs, that the chemical signal is turned into an electrical one that can be sent to the brain. First stop: the lateral geniculate nucleus (LGN) in the thalamus. The thalamus contains a whole slew of nuclei (collections of cells), each representing a different sense. (Example: the medial geniculate nucleus is for hearing.) The LGN then shoots the visual information up to primary visual cortex (V1) for basic processing. From V1 the information is sent to higher processing centers to extract more details about the visual scene and to form associations with previous memories.\nEach step in the hierarchy deals with a more complicated concept. Spots of light becomes lines, which become curves, etc. But the receptive field of the RGC is worth noting. These cells are very particular about the stiumulus they need to see in order to fire. Here, you can see how a so-called “off-center” RGC receptive field looks. This cell will fire maximally when there is light illuminated only in the “surround” portion of the receptive field, with the center remaining in darkness.\n","date":1588896000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588896000,"objectID":"e4e330737d6188456655248fb548da53","permalink":"/project/center-surround/","publishdate":"2020-05-08T00:00:00Z","relpermalink":"/project/center-surround/","section":"project","summary":"The back of your retina is lined with light-sensitive rods and cones. These photoreceptors catch the light and send the light information to bipolar cells, who then send it to retinal ganglion cells (RGCs).","tags":["science around us"],"title":"Center Surround","type":"project"},{"authors":[],"categories":[],"content":"\rThe Data: Billboard 200 Tracks\rThis data set is from Components One’s Datasets. It’s a large database containing data on 340,000 tracks from Billboard 200 albums released from 1963-2019. Included for each track is the following:\n\rTrack name\rTrack ID on Spotify\rAlbum name\rAlbum ID on Spotify\rArtist name\rDuration\rRelease date of the album\rSpotify’s EchoNest acoustic data:\r\rAcousticness\rDanceability\rEnergy\rInstrumentalness\rLiveness\rLoudness\rSpeechiness\rKey\rTime signature\rValence\r\r\r\rThe Visualization: How has the danceability of music changed over time?\rHere, color indicates how “danceable” music was for a given year. Immediately, we can tell that the ’80s and ’90s were the most danceable decades and that 1961 must have been pretty dark days for music.\n\rThe Details: How the Plot was Made\rThis ridgeline plot was made using the ggridges package, which integrates with ggplot. (For more information on the ridgeline plot, see my post.) Here, I used geom_density_ridges() and faceted by decade. The color gradient is the “inferno” palette from viridis. The theme is ggplot:theme_minimal() with some modifications to the background and facet titles.\nggplot(track_data, aes(x = danceability, y = year, fill = YearDanceability)) +\rgeom_density_ridges(scale = 4, alpha = 0.9) + facet_wrap(~ decade, scales = \u0026quot;free_y\u0026quot;, nrow = 1) + # Need free_y scale otherwise the plots aren\u0026#39;t aligned\rscale_y_discrete(expand = c(0,0,0, 4)) + # The facet labels were covering the top curve so I added padding\rscale_x_continuous(expand = c(0, 0)) +\rcoord_cartesian(clip = \u0026quot;off\u0026quot;) + # to avoid clipping of the top bit of the top curve\rscale_fill_viridis(option = \u0026quot;inferno\u0026quot;) +\rtheme_minimal() +\rlabs(fill = \u0026quot;Mean Year\\nDanceability\\nScore\u0026quot;,\rtitle = \u0026quot;1980\u0026#39;s \u0026amp; 90\u0026#39;s: The Most Danceable Decades?\u0026quot;,\rsubtitle = \u0026#39;How does the \u0026quot;danceability\u0026quot; of music change over time?\u0026#39;,\rx = \u0026quot;Song Danceability Score\u0026quot;,\ry = \u0026quot;Year\u0026quot;) +\rtheme(strip.background = element_rect(color = \u0026quot;white\u0026quot;, fill = \u0026quot;lightgray\u0026quot;), # Format the facet labels\raxis.title.y = element_text(hjust = 0.35)) # Change the justification of the y-axis label\r\r","date":1587513600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588964408,"objectID":"27ff4f306f95beba88803e9947d79938","permalink":"/post/danceability/","publishdate":"2020-04-22T00:00:00Z","relpermalink":"/post/danceability/","section":"post","summary":"The Data: Billboard 200 Tracks\rThis data set is from Components One’s Datasets. It’s a large database containing data on 340,000 tracks from Billboard 200 albums released from 1963-2019. Included for each track is the following:","tags":["data viz"],"title":"Danceability Across the Ages","type":"post"},{"authors":[],"categories":[],"content":"\r\rRidgeline plots are a variation of density plots in which you aim to compare the distributions of several categorical variables (represented on the y-axis) for a single continuous variable (represented on the x-axis). This is a quick way to compare a large number of groups where doing something like a simple geom_density() + facet_wrap() would occupy a large amount of space. By making use of transparency the ridges can be places in close proximity to save space.\nThis ridgeline plot was created using the package ggridges, which integrates with ggplot in R. Here I used geom_density_ridges(). There are ways to add more features to the ridges (ex. raincloud, rug). See the following resources for more information:\n\rCRAN vignette\n\rWilke’s Github\r\rThe Data: Billboard 200 Tracks\rThis data set is from Components One’s Datasets. It’s a large database containing data on 340,000 tracks from Billboard 200 albums released from 1963-2019. Included for each track is the following:\n\rTrack name\rTrack ID on Spotify\rAlbum name\rAlbum ID on Spotify\rArtist name\rDuration\rRelease date of the album\rSpotify’s EchoNest acoustic data:\r\rAcousticness\rDanceability\rEnergy\rInstrumentalness\rLiveness\rLoudness\rSpeechiness\rKey\rTime signature\rValence\r\r\rA glimpse of the data:\r\r\rSong\r\rArtist\r\rYear\r\rDecade\r\rSong Danceability\r\rYear Danceability\r\rDecade Danceability\r\r\r\r\r\rA\r\rCartel\r\r2005\r\r2000’s\r\r0.28\r\r0.55\r\r0.55\r\r\r\rA\r\rBarenaked Ladies\r\r1994\r\r1990’s\r\r0.78\r\r0.55\r\r0.57\r\r\r\rA-1 Performance\r\rAZ\r\r2002\r\r2000’s\r\r0.72\r\r0.57\r\r0.55\r\r\r\rA-11\r\rJamey Johnson\r\r2012\r\r2010’s\r\r0.73\r\r0.51\r\r0.53\r\r\r\rA-11\r\rBuck Owens\r\r1995\r\r1990’s\r\r0.55\r\r0.56\r\r0.57\r\r\r\r\r\r\rRepresentation Description\rI found the idea of “danceability” interesting; I wanted to see how danceability changed over time. There are a lot of years represented in this dataset. Initial data exploration showed that data for tracks before 1960 were much less abundant than other years (ex. there were 4 tracks from the 1930’s and 10,000+ from 1999 alone) so I decided to exclude data from before 1960. I was still left with a lot of data so I figured that the best way to quickly see a trend was through the use of color. What I’m trying to show in this graph is that the 1980’s and 90’s have higher danceability scores than other decades.\nOn the x-axis is the danceability score, ranging from 0 to 1. On the y-axis is each year (or decade). (In one iteration of this graph I’ve used facet_wrap() to split up the data by decade.) There is a continuous color scale used to encode for the average daceability rating of the given year (or decade). Transparency is used to allow for easier discernability of the underlying ridges. Each ridge represents the distribution of danceability scores for all tracks released that year.\n\rHow to interpret\rThe most danceable era should be that with the brightest (most yellow/white) ridges. Here it is clear that the 1980’s and ’90s are brightest. The peaks of the ridges should also line up somewhat continuously and show the trend for how danceability changes from year to year, in addition to the color trend (re: redundancy). A more subtle trend is that the variance within each year seems to become wider over time as well. This graph makes the identification of continuous trends as well as stark outliers quite easy.\n\rPresentation Tips\rTo draw attention to specific ridges, callout annotations can be used. Color can be used on a continuous scale to observe relative differences between ridges. Discrete colors can be used to compare between categorical variables with no ordinal relationship (in this case I suggest a color scale that does not imply order).\n\rVariations and Alternatives\rRidgeline plots are related to histograms, density plots, and violin plots. Compared to ridgeline plots these all have the disadvantage of taking up more space. Histograms bin the data into a given number of bins and therefore don’t have the smooth look of ridgeline plots but do indicate changes from bin to bin with more fidelity. Density plots are ridgeline plots that are either overplotted or separated into facets. They accomplish the same thing as ridgeline plots but in the case of overplotting there is no sense of change across variables and in the case of faceting they are more difficult to compare. Violin plots are very similar to the ridgeline plot, especially the half violin. Full violin plots would take up substantially more space than the ridgeline plot.\nThe Ridgeline Plot(s)\rDanceability by Decade\r# Filter to only include data after 1960\rdecadeplot \u0026lt;- ggplot(track_data %\u0026gt;% filter(year \u0026gt;= \u0026quot;1960\u0026quot;), aes(x = danceability, y = decade, fill = DecadeDanceability)) +\rgeom_density_ridges(scale = 4, alpha = 0.9, color = \u0026quot;red4\u0026quot;) +\rscale_y_discrete(expand = c(0, 0)) +\rscale_x_continuous(expand = c(0, 0)) +\rcoord_cartesian(clip = \u0026quot;off\u0026quot;) + # to avoid clipping of the top bit of the top curve\rscale_fill_viridis(option = \u0026quot;inferno\u0026quot;) +\rtheme_minimal() +\rlabs(fill = \u0026quot;Mean Decade\\nDanceability\\nScore\u0026quot;,\rtitle = \u0026quot;1980\u0026#39;s \u0026amp; 90\u0026#39;s: The Most Danceable Decades?\u0026quot;,\rsubtitle = \u0026#39;How does the \u0026quot;danceability\u0026quot; of music change over time?\u0026#39;,\rx = \u0026quot;Song Danceability Score\u0026quot;,\ry = \u0026quot;Decade\u0026quot;) +\rtheme(axis.title.y = element_text(hjust = 0.25)) # Move y lable to be in the center of the y-axis labels, not the whole y-axis\rdecadeplot\r\rDanceability by Year: Faceted by Decade\rggplot(track_data %\u0026gt;% filter(year \u0026gt;= \u0026quot;1960\u0026quot;), aes(x = danceability, y = year, fill = YearDanceability)) +\rgeom_density_ridges(scale = 4, alpha = 0.9, color = \u0026quot;red4\u0026quot;) + facet_wrap(~ decade, scales = \u0026quot;free_y\u0026quot;, nrow = 1) + # Need free_y scale otherwise the plots would not be aligned\rscale_y_discrete(expand = c(0,0,0, 4)) + # The facet labels were covering the top of the top curve so I introduced some padding\rscale_x_continuous(expand = c(0, 0)) +\rcoord_cartesian(clip = \u0026quot;off\u0026quot;) + # to avoid clipping of the top bit of the top curve\rscale_fill_viridis(option = \u0026quot;inferno\u0026quot;) +\rtheme_minimal() +\rlabs(fill = \u0026quot;Mean Year\\nDanceability\\nScore\u0026quot;,\rtitle = \u0026quot;1980\u0026#39;s \u0026amp; 90\u0026#39;s: The Most Danceable Decades?\u0026quot;,\rsubtitle = \u0026#39;How does the \u0026quot;danceability\u0026quot; of music change over time?\u0026#39;,\rx = \u0026quot;Song Danceability Score\u0026quot;,\ry = \u0026quot;Year\u0026quot;) +\rtheme(strip.background = element_rect(color = \u0026quot;white\u0026quot;, fill = \u0026quot;lightgray\u0026quot;), # Format the facel labels\raxis.title.y = element_text(hjust = 0.35)) # Change the justification of the y-axis label\r\rDanceability by Year\rggplot(track_data %\u0026gt;% filter(decade \u0026gt;= \u0026quot;1960\u0026quot;), aes(x = danceability, y = year, fill = YearDanceability)) +\rgeom_density_ridges(scale = 4, alpha = 0.9, color = \u0026quot;red4\u0026quot;) + scale_y_discrete(expand = c(0, 0)) +\rscale_x_continuous(expand = c(0, 0)) +\rcoord_cartesian(clip = \u0026quot;off\u0026quot;) + # to avoid clipping of the top bit of the top curve\rscale_fill_viridis(option = \u0026quot;inferno\u0026quot;) +\rtheme_minimal() +\rlabs(fill = \u0026quot;Mean Year\\nDanceability\\nScore\u0026quot;,\rtitle = \u0026quot;1980\u0026#39;s \u0026amp; 90\u0026#39;s: The Most Danceable Decades?\u0026quot;,\rsubtitle = \u0026#39;How does the \u0026quot;danceability\u0026quot; of music change over time?\u0026#39;,\rx = \u0026quot;Song Danceability Score\u0026quot;,\ry = \u0026quot;Year\u0026quot;)\r\r\r\r","date":1587513600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587513600,"objectID":"5ebf510cba7def2a7901b51b301479af","permalink":"/project/the-ridgeline-plot/","publishdate":"2020-04-22T00:00:00Z","relpermalink":"/project/the-ridgeline-plot/","section":"project","summary":"Ridgeline plots are a variation of density plots in which you aim to compare the distributions of several categorical variables (represented on the y-axis) for a single continuous variable (represented on the x-axis).","tags":["data viz","how to"],"title":"The Ridgeline Plot","type":"project"},{"authors":[],"categories":[],"content":"\rThe Data: Personal Fitbit Heart Rate Data\rI’ve been wearing a Fitbit for nearly two years at this point and as a result, I have a lot of data on my heart rate, sleep, and activity. Fitbit allows you to export all of your data fairly easily. I then did some basic data wrangling to isolate my “before PE” and “after PE” heart rate data.\n\rThe Visualization: How was my heart rate affected by my embolism?\rAs the plot states, I had a pulmonary embolism last July. Being the data nerd that I am, ever since my embolism I have been obsessed with tracking my heart rate using my Fitbit. Here I used color to separate my heart rate data from before my PE to my heart rate after my PE It’s clear that my heart rate has been elevated following my PE.\n\rThe Details: How the Plot was Made\rThis plot was made in ggplot2 using geom_line(). The theme used is ggplot2::theme_light() with some simple tweaks. The color palette was made using the beyonce package. Here I pulled two colors from the #78 color palette:\nggplot(fitbit, aes(x = Week, y = BPM, group = `Heart Rate`, color = `Heart Rate`)) +\rgeom_line(size = 1) +\rlabs(y = \u0026quot;Resting Heart Rate (BPM)\u0026quot;,\rtitle = \u0026quot;Resting Heart Rate after Pulmonary Embolism\u0026quot;,\rsubtitle = \u0026quot;In July 2019 I had a pulmonary embolism (PE). My heart rate increased as a result. \\nHere is my resting heart rate six months before my PE and six months after my PE.\u0026quot;) +\rscale_x_continuous(expand = c(0.01,0.01), breaks = seq(0, 26, 5)) +\rtheme_light() +\rtheme(panel.border = element_blank(),\rplot.title = element_text(face = \u0026quot;bold\u0026quot;, size = 15)) +\rscale_color_manual(values = beyonce_palette(78)[c(2,4)])\r\r","date":1586908800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588954656,"objectID":"94647e333d35d05d4dd147be7d1670a8","permalink":"/post/hr-after-pe/","publishdate":"2020-04-15T00:00:00Z","relpermalink":"/post/hr-after-pe/","section":"post","summary":"The Data: Personal Fitbit Heart Rate Data\rI’ve been wearing a Fitbit for nearly two years at this point and as a result, I have a lot of data on my heart rate, sleep, and activity.","tags":["data viz"],"title":"Resting Heart Rate after Pulmonary Embolism","type":"post"},{"authors":[],"categories":[],"content":"\rThe Data: The New York MoMA Collection\rFrom Data Is Plural:\n\rThis July, the Museum of Modern Art published a dataset containing 120,000 artworks from its catalog, joining the UK’s Tate, the Smithsonian’s Cooper Hewitt, and other forward-thinking museums. The MoMA data contains the names of the artwork and artist, the dates created and acquired, and the medium — but no images. Related: Artist Jer Thorp encourages you to “perform” the data. Also related: Every museum in the United States. (h/t Nadja Popovich)\n\rThe data can be found here.\n\rThe Visualization: Is there a difference in lifespan between male and female artists?\rThis plot looks at the lifespans of the artists represented in the MoMA collection of paintings. The distribution of lifespans for males and females are overalayed with the average lifespan for each gender. Through this visualization we can see that male and female artists had similar lifespans with the average lifespan for both gender being around 75 years of age.\n\rThe Details: How the Plot was Made\rThis plot was made in ggplot2 using geom_density(). To add the vertical lines I used geom_vline(), setting the x-intercept to the average lifespan, calculated elsewhere. The text and arrow were created using the annotate() function with geom = \"text\" and geom = \"curve\". The theme for this plot is theme_fivethirtyeight() from the ggthemes package, with some modifications to the legend and axes.\nmoma_life \u0026lt;- moma %\u0026gt;% mutate(Lifespan = artist_death_year - artist_birth_year) %\u0026gt;% filter(!is.na(Lifespan)) %\u0026gt;%\rfilter(!is.na(artist_gender))\r# Calculate the average lifespans for each gender\ravgmale \u0026lt;- moma_life %\u0026gt;% filter(artist_gender == \u0026quot;Male\u0026quot;)\ravgmale \u0026lt;- mean(avgmale$Lifespan)\ravgfemale \u0026lt;- moma_life %\u0026gt;% filter(artist_gender == \u0026quot;Female\u0026quot;)\ravgfemale \u0026lt;- mean(avgfemale$Lifespan)\r# Set color palette\rcolorgender \u0026lt;- c(\u0026quot;Male\u0026quot; = \u0026quot;lightseagreen\u0026quot;, \u0026quot;Female\u0026quot; = \u0026quot;indianred1\u0026quot;)\r# The plot\rggplot(data = moma_life, aes(fill = artist_gender, x = Lifespan)) +\rgeom_density(alpha = 0.6, size = 1) +\rcoord_cartesian(xlim = c(27,102)) +\rgeom_vline(xintercept = avgmale, size = 1, color = \u0026quot;turquoise4\u0026quot;) +\rgeom_vline(xintercept = avgfemale, size = 1, color = \u0026quot;indianred1\u0026quot;) +\rtheme_fivethirtyeight() +\rscale_fill_manual(values = colorgender) +\rtheme(legend.position = c(0.115, 0.95), legend.title = element_blank(),\rlegend.background = element_rect(),\rlegend.margin = margin(c(0,0,7,0)),\raxis.title = element_text()) +\rlabs(title = \u0026quot;The lifespan of MoMA artists\u0026quot;,\rsubtitle = \u0026quot;Here we see the distribution of lifepans for male and female artists \\nrepresented in the MoMA collection\u0026quot;,\rx = \u0026quot;Lifespan (Years)\u0026quot;,\ry = \u0026quot;Frequency of Occurrence\u0026quot;) +\rannotate(x = 77, y = 0.004, geom = \u0026quot;text\u0026quot;,\rlabel = \u0026quot;average \\nlifespans\u0026quot;, color = \u0026quot;grey20\u0026quot;, size = 4,\rhjust = 0, fontface = 2, lineheight = 0.8) +\rannotate(geom = \u0026quot;curve\u0026quot;, size = 1, color = \u0026quot;grey20\u0026quot;,\rx = 80.5, y = 0.006, xend = 76, yend = 0.009,\rarrow = arrow(length = unit(3, \u0026quot;mm\u0026quot;)))\r\r","date":1586304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588996858,"objectID":"5e929d2ea2e82cb5e11206359ba7b7bb","permalink":"/post/lifespan-of-moma-artists/","publishdate":"2020-04-08T00:00:00Z","relpermalink":"/post/lifespan-of-moma-artists/","section":"post","summary":"The Data: The New York MoMA Collection\rFrom Data Is Plural:\n\rThis July, the Museum of Modern Art published a dataset containing 120,000 artworks from its catalog, joining the UK’s Tate, the Smithsonian’s Cooper Hewitt, and other forward-thinking museums.","tags":["data viz"],"title":"The lifespan of MoMA artists","type":"post"},{"authors":[],"categories":[],"content":"\r\n\r\n\r\n\r\n","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585699200,"objectID":"5127bb92b38dc67fb6a2dd1b2a4b4335","permalink":"/project/microscope-me-up-purkinje-cells/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/project/microscope-me-up-purkinje-cells/","section":"project","summary":"","tags":["sci comm"],"title":"Microscope Me Up: Purkinje Cells","type":"project"},{"authors":[],"categories":[],"content":"\r\n\r\n\r\n\r\n","date":1567382400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567382400,"objectID":"c20c09f7af0bc52cfd0d49b62126f318","permalink":"/project/soph-talks-science-sep-2019/","publishdate":"2019-09-02T00:00:00Z","relpermalink":"/project/soph-talks-science-sep-2019/","section":"project","summary":"","tags":["sci comm"],"title":"Soph Talks Science: Cellfie of the Month Sept 2019","type":"project"}]